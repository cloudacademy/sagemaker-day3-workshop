{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EbYruzKRewzf"},"source":["# Day 3 – Workshop 5: In-Context Learning & Advanced Techniques\n","\n","**Objective:** Explore *few-shot prompting*, *chain-of-thought*, and a minimal *retrieval-augmented generation (RAG)* workflow. We’ll also **compare** standard prompts (no RAG) vs. RAG-based prompts to see the difference.\n","\n","---\n","## Table of Contents\n","1. [Introduction & Recap](#Intro)\n","2. [Few-Shot & Chain-of-Thought Prompting](#FewShot)\n","3. [Retrieval-Augmented Generation (RAG) Intro](#RAG)\n","4. [Mini RAG Lab](#MiniRAG)\n","5. [Comparison: With RAG vs. Without RAG](#Compare)\n","6. [Optional: Gradio Web Interface](#Gradio)\n","7. [Discussion & Next Steps](#Discussion)\n","\n","---"],"id":"EbYruzKRewzf"},{"cell_type":"markdown","metadata":{"id":"ESkBNnz2ewzh"},"source":["<a id=\"Intro\"></a>\n","\n","## 1. Introduction & Recap\n","Building on **Workshop 4**’s prompt engineering:\n","- We now **strengthen** LLM responses by providing **few-shot** examples or a **chain-of-thought** approach.\n","- We also introduce **RAG**: retrieve context from an external store to feed the LLM domain-specific knowledge.\n","- Finally, we’ll compare standard prompting vs. RAG-based prompting to highlight differences.\n","\n","**Requirements**:\n","- A moderately sized model loaded (e.g., GPT-Neo).  \n","- `sentence-transformers` for RAG (or an equivalent for embeddings).  \n","- *(Optional)* `gradio` for a web UI.\n"],"id":"ESkBNnz2ewzh"},{"cell_type":"markdown","metadata":{"id":"_Du9D6YCewzi"},"source":["<a id=\"FewShot\"></a>\n","\n","## 2. Few-Shot & Chain-of-Thought Prompting\n","A **few-shot prompt** includes multiple examples. A **chain-of-thought** prompt explicitly instructs the model to outline reasoning. We'll assume you have a loaded model and tokenizer from Workshop 4 or similar. If not, include the code:\n","```python\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"google/flan-t5-base\"  # or your choice\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n","```"],"id":"_Du9D6YCewzi"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ethT9FG6ewzj","outputId":"25bbd0bf-db90-4511-90ca-53fd8b9bdc88","executionInfo":{"status":"ok","timestamp":1742515991209,"user_tz":0,"elapsed":5950,"user":{"displayName":"S West","userId":"02976195163573653786"}}},"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers import pipeline\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","gen_model_name = \"google/flan-t5-base\"\n","gen_pipeline = pipeline(\"text2text-generation\", model=gen_model_name)\n","\n","def generate_llm_text(prompt, max_length=100, temperature=0.7):\n","  \"\"\"Generates text using the specified pipeline.\"\"\"\n","\n","  # Use the pipeline for text generation\n","  sequences = gen_pipeline(prompt, max_length=max_length, do_sample=True, temperature=temperature)\n","\n","  # Extract and return the generated text\n","  if sequences:\n","    return sequences[0]['generated_text']\n","  else:\n","    return \"\"  # Handle cases where no text is generated\n","\n","\n","print(\"Model loaded successfully!\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded successfully!\n"]}],"id":"ethT9FG6ewzj"},{"cell_type":"code","source":[],"metadata":{"id":"ZZSBZuQV3cbS","executionInfo":{"status":"ok","timestamp":1742515991219,"user_tz":0,"elapsed":6,"user":{"displayName":"S West","userId":"02976195163573653786"}}},"id":"ZZSBZuQV3cbS","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"suC-Dl7Lm_wT"},"id":"suC-Dl7Lm_wT"},{"cell_type":"markdown","metadata":{"id":"i3erjpWCewzj"},"source":["### Few-Shot Prompt Example\n","We supply multiple examples to guide the model."],"id":"i3erjpWCewzj"},{"cell_type":"code","metadata":{"id":"QuMI_prfewzk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742515992131,"user_tz":0,"elapsed":910,"user":{"displayName":"S West","userId":"02976195163573653786"}},"outputId":"8942e1b1-32ec-4591-ef51-615382ddd5f2"},"source":["few_shot_prompt = \"\"\"\n","Q: How can companies reduce customer churn?\n","A: They can improve onboarding, offer loyalty benefits, and proactively address user feedback.\n","\n","Q: How can manufacturers reduce production delays?\n","A:\"\"\"\n","\n","response_fs = generate_llm_text(few_shot_prompt, max_length=120)\n","print(\"=== Few-Shot Prompt Output ===\\n\")\n","print(response_fs)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Few-Shot Prompt Output ===\n","\n","The more products are completed, the longer they can keep the product or processes running smoothly.\n"]}],"id":"QuMI_prfewzk"},{"cell_type":"markdown","metadata":{"id":"5YSCyQm8ewzl"},"source":["### Chain-of-Thought Prompt Example\n","We explicitly ask the model to detail its reasoning steps."],"id":"5YSCyQm8ewzl"},{"cell_type":"code","metadata":{"id":"tob68875ewzl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742515992457,"user_tz":0,"elapsed":325,"user":{"displayName":"S West","userId":"02976195163573653786"}},"outputId":"316b1a61-3e86-49ca-d76c-c840b5c34037"},"source":["cot_prompt = \"\"\"\n","Question: If I have 8 apples and I give away 3, how many do I have left?\n","Let’s think this through step by step:\n","\"\"\"\n","chain_output = generate_llm_text(cot_prompt, max_length=80)\n","print(\"=== Chain-of-Thought Output ===\\n\")\n","print(chain_output)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Chain-of-Thought Output ===\n","\n","I have 8 + 3 = 10 apples left. The answer: 10.\n"]}],"id":"tob68875ewzl"},{"cell_type":"markdown","metadata":{"id":"ithaIlzPewzm"},"source":["<a id=\"RAG\"></a>\n","\n","## 3. Retrieval-Augmented Generation (RAG) Intro\n","RAG merges **retrieval** of external data with an LLM, so the model has **domain-specific** context.\n","We’ll create a tiny in-memory example with `sentence-transformers` for embeddings."],"id":"ithaIlzPewzm"},{"cell_type":"code","metadata":{"id":"vACkG_wGewzm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742515997376,"user_tz":0,"elapsed":4914,"user":{"displayName":"S West","userId":"02976195163573653786"}},"outputId":"32ed6e5a-fb9d-43dd-c876-2475b197cc99"},"source":["# !pip install sentence-transformers\n","from sentence_transformers import SentenceTransformer, util\n","\n","# Example mini corpus\n","faqs = [\n","    {\"question\": \"How do I reset my password?\", \"answer\": \"Click 'Forgot Password' on the login screen.\"},\n","    {\"question\": \"How can I contact support?\", \"answer\": \"Email us at support@example.com.\"},\n","    {\"question\": \"Where do I view my account details?\", \"answer\": \"Log in, then go to the 'My Account' page.\"},\n","    {\"question\": \"What is the refund policy?\", \"answer\": \"You can request a refund within 30 days.\"},\n","    {\"question\": \"How do I change my email address?\", \"answer\": \"Update your email from the 'Profile Settings' section.\"},\n","    {\"question\": \"How can I track my order?\", \"answer\": \"Use the tracking link in your confirmation email or check 'Order History'.\"},\n","    {\"question\": \"How do I update my billing information?\", \"answer\": \"Visit 'Billing & Payment' in your account settings to update your card details.\"},\n","    {\"question\": \"How do I unsubscribe from marketing emails?\", \"answer\": \"Click the 'Unsubscribe' link at the bottom of any promotional email.\"},\n","    {\"question\": \"Where can I find the user guide?\", \"answer\": \"You’ll find the guide in the 'Help Centre' on our website.\"},\n","    {\"question\": \"Why can’t I log in?\", \"answer\": \"Ensure you’re using the correct credentials or reset your password if needed.\"},\n","    {\"question\": \"Where do I find my order history?\", \"answer\": \"Go to your account dashboard and select 'Order History'.\"},\n","    {\"question\": \"What payment methods are accepted?\", \"answer\": \"We accept major credit cards, PayPal, and direct bank transfers.\"},\n","    {\"question\": \"How do I change my profile picture?\", \"answer\": \"Upload a new image under 'Profile Settings' in your account.\"},\n","    {\"question\": \"Can I pause my subscription?\", \"answer\": \"Yes, visit the 'Subscription' page and select 'Pause Subscription'.\"},\n","    {\"question\": \"How do I enable two-factor authentication (2FA)?\", \"answer\": \"Navigate to 'Security Settings' and follow the steps to enable 2FA.\"},\n","    {\"question\": \"What is the cancellation process for the service?\", \"answer\": \"You can cancel anytime from the 'Account Settings' page.\"},\n","    {\"question\": \"How do I restore a deleted item?\", \"answer\": \"Check the 'Recently Deleted' folder or contact support if it’s not there.\"},\n","    {\"question\": \"Which browsers are supported?\", \"answer\": \"We support Chrome, Firefox, Safari, and the latest version of Edge.\"},\n","    {\"question\": \"Is phone support available?\", \"answer\": \"Yes, our helpline number is listed on the 'Contact Us' page.\"},\n","    {\"question\": \"What is the scheduled downtime for maintenance?\", \"answer\": \"We post maintenance schedules in advance on the 'System Status' page.\"},\n","    {\"question\": \"How do I create an account?\", \"answer\": \"Click 'Sign Up' on the homepage and fill in the required information.\"},\n","    {\"question\": \"Where can I read about the latest features?\", \"answer\": \"Visit our 'Blog' or check the 'Release Notes' in your account.\"},\n","    {\"question\": \"How do I update my mailing address?\", \"answer\": \"Go to 'Account Settings', then click 'Edit' under 'Mailing Address'.\"},\n","    {\"question\": \"What do I do if I encounter error code 403?\", \"answer\": \"Clear your cache, check your login status, and contact support if the error persists.\"}\n","]\n","embed_model = SentenceTransformer('all-mpnet-base-v2')\n","faq_embeddings = []\n","for i, item in enumerate(faqs):\n","    emb = embed_model.encode(item[\"question\"], convert_to_tensor=True)\n","    faq_embeddings.append({\n","        \"index\": i,\n","        \"question\": item[\"question\"],\n","        \"answer\": item[\"answer\"],\n","        \"embedding\": emb\n","    })\n","\n","print(\"Loaded FAQ embeddings.\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded FAQ embeddings.\n"]}],"id":"vACkG_wGewzm"},{"cell_type":"markdown","metadata":{"id":"IQ7dSvAEewzn"},"source":["<a id=\"MiniRAG\"></a>\n","\n","## 4. Mini RAG Lab\n","Retrieve the top matching FAQ and append to the LLM prompt."],"id":"IQ7dSvAEewzn"},{"cell_type":"code","metadata":{"id":"ZhyuLU1rewzn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742515997794,"user_tz":0,"elapsed":420,"user":{"displayName":"S West","userId":"02976195163573653786"}},"outputId":"568fd673-eaeb-46e0-81e9-06f0ea863792"},"source":["def retrieve_faq_context(user_query, top_k=1):\n","    query_emb = embed_model.encode(user_query, convert_to_tensor=True)\n","    scores = []\n","    for entry in faq_embeddings:\n","        sim = util.pytorch_cos_sim(query_emb, entry[\"embedding\"]).item()\n","        scores.append((sim, entry))\n","\n","    # Sort descending by similarity\n","    scores = sorted(scores, key=lambda x: x[0], reverse=True)\n","\n","    # Return the top_k FAQ entries\n","    top_matches = scores[:top_k]\n","    return [m[1] for m in top_matches]\n","\n","def debug_faq_retrieval(user_query):\n","    query_emb = embed_model.encode(user_query, convert_to_tensor=True)\n","    all_scores = []\n","\n","    for faq in faq_embeddings:\n","        sim = util.pytorch_cos_sim(query_emb, faq[\"embedding\"]).item()\n","        all_scores.append((sim, faq[\"question\"], faq[\"answer\"]))\n","\n","    # Sort by descending similarity\n","    all_scores = sorted(all_scores, key=lambda x: x[0], reverse=True)\n","\n","    print(f\"Query: {user_query}\\n\")\n","    for score, question, answer in all_scores:\n","        print(f\"Similarity: {score:.4f} | Q: {question} | A: {answer}\")\n","\n","def rag_query(user_query):\n","    # retrieve\n","    top_ctx = retrieve_faq_context(user_query, top_k=1)[0]\n","    context_str = f\"Q: {top_ctx['question']} A: {top_ctx['answer']}\"\n","\n","    prompt = f\"\"\"\n","You are a helpful assistant.\n","Answer this User's question: {user_query}.\n","Here is some information to help you:\n","{context_str}\n","\n","\n","\n","Answer:\n","\"\"\"\n","    return generate_llm_text(prompt, max_length=100, temperature=0.2)\n","\n","# Example usage\n","test_query = \"How do I enable two-factor authentication (2FA)?\"\n","print(\"=== RAG Answer ===\\n\")\n","print(rag_query(test_query))\n","\n","#debug_faq_retrieval(\"How do I enable two-factor authentication (2FA)?\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["=== RAG Answer ===\n","\n","'Security Settings' is the section that contains the security settings for your device.\n"]}],"id":"ZhyuLU1rewzn"},{"cell_type":"markdown","metadata":{"id":"wtVSCQYjewzn"},"source":["### Discussion\n","- Compare the output with or without RAG.\n","- In production, use a **vector DB** like Pinecone, FAISS, or Milvus with a larger text corpus."],"id":"wtVSCQYjewzn"},{"cell_type":"markdown","metadata":{"id":"J5SoTp3Vewzo"},"source":["<a id=\"Compare\"></a>\n","\n","## 5. Comparison: With RAG vs. Without RAG\n","Let’s explicitly show how an **LLM responds** to the same user query in two scenarios:\n","1. **No context**: Just the user query.\n","2. **RAG**: The same user query, but augmented with the relevant FAQ.\n","\n","### Activity\n","Try queries that might benefit from domain-specific knowledge. Compare the difference in output."],"id":"J5SoTp3Vewzo"},{"cell_type":"code","metadata":{"tags":["activity"],"id":"06EjAWZNewzo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742516026642,"user_tz":0,"elapsed":1018,"user":{"displayName":"S West","userId":"02976195163573653786"}},"outputId":"823388bd-8eca-4b92-f6ef-0de57e6cd362"},"source":["user_query = \"How do I enable two-factor authentication (2FA)?\"\n","\n","# 1) No context\n","no_context_answer = generate_llm_text(user_query)\n","\n","# 2) RAG-based\n","rag_answer = rag_query(user_query)\n","\n","print(\"=== No Context ===\\n\")\n","print(no_context_answer)\n","print(\"\\n=== RAG-Based ===\\n\")\n","print(rag_answer)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["=== No Context ===\n","\n","Type the following into your web browser: f:config.flush:stdout.write:stdout.write\n","\n","=== RAG-Based ===\n","\n","Open the Settings menu and click on 'Security Settings'.\n"]}],"id":"06EjAWZNewzo"},{"cell_type":"markdown","metadata":{"id":"cm6Uc9Zcewzo"},"source":["*Note:* The RAG-based approach typically references the retrieved FAQ, so you’re more likely to get an accurate, domain-specific answer."],"id":"cm6Uc9Zcewzo"},{"cell_type":"markdown","metadata":{"id":"-QESkaWrewzo"},"source":["<a id=\"Gradio\"></a>\n","\n","## 6. Optional: Gradio Web Interface\n","Try a web UI for **advanced** prompting or RAG queries. You can select your approach (chain-of-thought or RAG) in the callback function.\n"],"id":"-QESkaWrewzo"},{"cell_type":"code","metadata":{"id":"tBeKPvWHewzo","colab":{"base_uri":"https://localhost:8080/","height":680},"executionInfo":{"status":"ok","timestamp":1742516107286,"user_tz":0,"elapsed":31195,"user":{"displayName":"S West","userId":"02976195163573653786"}},"outputId":"0a9e16da-e345-4309-90f1-11a71dd11b3d"},"source":["#!pip install gradio\n","import gradio as gr\n","\n","def gradio_rag(query):\n","    return rag_query(query)\n","\n","demo = gr.Interface(\n","    fn=gradio_rag,\n","    inputs=\"text\",\n","    outputs=\"text\",\n","    title=\"Workshop 5: Advanced Techniques (RAG)\",\n","    description=\"Enter a query, we'll retrieve an FAQ and generate a domain-specific response.\"\n",")\n","# Uncomment to launch the UI:\n","demo.launch(debug=True)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://9b943cd6b37f6351e7.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://9b943cd6b37f6351e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://9b943cd6b37f6351e7.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":12}],"id":"tBeKPvWHewzo"},{"cell_type":"markdown","metadata":{"id":"Hz_nAEbuewzp"},"source":["<a id=\"Discussion\"></a>\n","\n","## 7. Discussion & Next Steps\n","In this workshop, you:\n","- Explored **few-shot** and **chain-of-thought** prompting in more depth.\n","- Saw how **RAG** can feed domain-specific context to an LLM.\n","- Compared **No-Context vs. RAG** side by side.\n","- Optionally integrated Gradio for an interactive interface.\n","\n","**Next Steps**:\n","- Expand your RAG approach with a real vector DB.\n","- Try more examples for chain-of-thought.\n","- **Workshop 6**: apply these advanced techniques in a final **prototype**.\n","\n","---\n","# End of Day 3 – Workshop 5 Notebook"],"id":"Hz_nAEbuewzp"}]}