{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uAT29gtWe8dT"},"source":["# Day 3 – Workshop 5 (Production-Like RAG Demo)\n","\n","**Objective:** Demonstrate a *more production-like* Retrieval-Augmented Generation (RAG) approach using:\n","1. A **larger open-source LLM** (GPU recommended),\n","2. An **external Hugging Face dataset** to build a small text corpus,\n","3. **FAISS** as a local vector store,\n","4. **Console tests** comparing prompt outputs *with* and *without* RAG,\n","5. A final **Gradio** UI for **interactive chat** against the RAG pipeline.\n","\n","---\n","## Table of Contents\n","1. [Setup & Dependencies](#Setup)\n","2. [Load a Powerful Open LLM](#LLM)\n","3. [Build the Corpus from a Hugging Face Dataset](#Corpus)\n","4. [Create FAISS Vector Store](#FAISS)\n","5. [RAG Pipeline Functions](#Pipeline)\n","6. [Console Test: Compare No-RAG vs. RAG](#Compare)\n","7. [Gradio UI: Interactive RAG Chat](#Gradio)\n","8. [Wrap-Up](#WrapUp)\n","\n","---"],"id":"uAT29gtWe8dT"},{"cell_type":"markdown","metadata":{"id":"iY_TTqshe8dV"},"source":["<a id=\"Setup\"></a>\n","\n","## 1. Setup & Dependencies\n","```\n","pip install datasets sentence-transformers faiss-cpu transformers torch gradio\n","```\n","**Important**:\n","- A **GPU** environment (e.g., at least 16 GB VRAM) is recommended for large models.\n","- We’ll demonstrate **FAISS** locally. In real production, you might use a hosted vector DB (Pinecone, Milvus, Weaviate, etc.).\n","\n","**Reflection**:\n","- *Which text dataset is relevant to your domain?*\n","- *Do you need an even larger model? Or can a smaller one suffice for cost/performance reasons?*\n","\n","Below, we filter out any annoying warning messages that might pop up when using certain libraries. Your code will run the same without doing this, we just prefer a cleaner output."],"id":"iY_TTqshe8dV"},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"ByWRMkPgw2tF"},"id":"ByWRMkPgw2tF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install datasets sentence-transformers faiss-cpu transformers torch gradio"],"metadata":{"id":"X1uqM6lrwTh8"},"id":"X1uqM6lrwTh8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCxAb1are8dW"},"source":["import torch\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", device)"],"execution_count":null,"outputs":[],"id":"oCxAb1are8dW"},{"cell_type":"markdown","metadata":{"id":"eIq1aCCIe8dX"},"source":["<a id=\"LLM\"></a>\n","\n","## 2. Load a Powerful Open LLM  \n","Below we show an example using **Flan-T5**, an open-source instruction-tuned model developed by Google Research. It supports a wide range of NLP tasks and is particularly well-suited for zero-shot and few-shot prompting.\n","\n","> **Note**: The size of the Flan-T5 model you choose (e.g., `flan-t5-base`, `flan-t5-large`, `flan-t5-xl`, or `flan-t5-xxl`) will affect memory requirements. For example, `flan-t5-xl` typically requires ~13GB VRAM in FP16. If you’re working with limited resources, consider using `flan-t5-base` or `flan-t5-large`, or explore quantised versions for more efficient inference.\n"],"id":"eIq1aCCIe8dX"},{"cell_type":"code","metadata":{"id":"A8FLGrF-e8dX"},"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers import pipeline\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","gen_model_name = \"\"HuggingFaceH4/zephyr-7b-beta\"\n","gen_pipeline = pipeline(\"text2text-generation\", model=gen_model_name)\n","\n","def generate_llm_text(prompt, max_length=150, temperature=0.7):\n","\n","  sequences = gen_pipeline(prompt, max_length=max_length, do_sample=True, temperature=temperature)\n","\n","  if sequences:\n","    return sequences[0]['generated_text']\n","  else:\n","    return \"\"\n","\n","\n","print(\"Model loaded successfully!\")"],"execution_count":null,"outputs":[],"id":"A8FLGrF-e8dX"},{"cell_type":"markdown","metadata":{"id":"FZ1I5BwVe8dX"},"source":["<a id=\"Corpus\"></a>\n","\n","## 3. Build the Corpus from a Hugging Face Dataset  \n","We’ll load a subset of the **arXiv dataset** from Hugging Face to serve as our document corpus. This dataset contains scientific papers from arXiv.org and is well-suited for tasks involving technical or academic text.\n","\n","```bash\n","pip install datasets\n"],"id":"FZ1I5BwVe8dX"},{"cell_type":"code","source":["%%capture\n","!pip install datasets"],"metadata":{"id":"OhSgHoPGr5AP"},"id":"OhSgHoPGr5AP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BEPHSz_ke8dY"},"source":["from datasets import load_dataset\n","\n","# Load the \"arxiv\" subset of the scientific_papers dataset\n","dataset = load_dataset(\"scientific_papers\", \"arxiv\", split=\"train\", trust_remote_code=True)\n","\n","print(\"Dataset size:\", len(dataset))\n","print(dataset[0])\n","\n","# We'll create a smaller corpus by filtering out empty or very short abstracts and limiting the total docs.\n","docs = []\n","for item in dataset:\n","    # Use the \"abstract\" field from the dataset\n","    text = item[\"abstract\"].strip()\n","    if len(text) > 30:  # skip very short abstracts\n","        docs.append(text)\n","    if len(docs) >= 500:  # limit to 500 abstracts for the demo\n","        break\n","\n","print(f\"Using {len(docs)} documents in our corpus.\")\n"],"execution_count":null,"outputs":[],"id":"BEPHSz_ke8dY"},{"cell_type":"markdown","metadata":{"id":"XfItvroWe8dY"},"source":["<a id=\"FAISS\"></a>\n","\n","## 4. Create FAISS Vector Store\n","We’ll use [FAISS](https://github.com/facebookresearch/faiss) locally to build an index of **document embeddings**.\n","\n","```bash\n","pip install faiss-cpu\n","```\n","\n","**Steps**:\n","1. Use **sentence-transformers** (or any embedding model) to embed each doc.\n","2. Create a **FAISS index**.\n","3. Store metadata so we can retrieve which doc was matched.\n"],"id":"XfItvroWe8dY"},{"cell_type":"code","source":["%%capture\n","!pip install faiss-cpu"],"metadata":{"id":"eunGhRrrshTW"},"id":"eunGhRrrshTW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"u1t4Lupne8dZ"},"source":["import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","\n","embed_model = SentenceTransformer('all-mpnet-base-v2', device=device)  # Chosen for strong semantic similarity performance on technical text\n","\n","def create_faiss_index(documents):\n","    # Convert each doc to an embedding\n","    embeddings = embed_model.encode(documents, show_progress_bar=True)\n","    embeddings = np.array(embeddings, dtype=\"float32\")\n","\n","    # Create FAISS index (using L2 or IP similarity)\n","    index = faiss.IndexFlatIP(embeddings.shape[1])\n","\n","    # Add embeddings to the index\n","    index.add(embeddings)\n","    return index, embeddings\n","\n","faiss_index, doc_embeddings = create_faiss_index(docs)\n","print(\"FAISS index created with\", faiss_index.ntotal, \"vectors.\")"],"execution_count":null,"outputs":[],"id":"u1t4Lupne8dZ"},{"cell_type":"markdown","metadata":{"id":"Cq1ahTgOe8dZ"},"source":["### Retrieving Similar Documents\n","We’ll define a helper to do nearest-neighbor search in FAISS."],"id":"Cq1ahTgOe8dZ"},{"cell_type":"code","metadata":{"id":"tIckcyOae8dZ"},"source":["def faiss_search(query, k=1):\n","    # embed query\n","    q_emb = embed_model.encode([query], show_progress_bar=False)\n","    q_emb = np.array(q_emb, dtype=\"float32\")\n","    # search\n","    distances, indices = faiss_index.search(q_emb, k)\n","    # retrieve docs\n","    results = []\n","    for idx in indices[0]:\n","        results.append(docs[idx])\n","    return results\n","\n","test_search = faiss_search(\"Recent advances in quantum computing\", k=2)\n","print(\"Sample retrieval:\", test_search)"],"execution_count":null,"outputs":[],"id":"tIckcyOae8dZ"},{"cell_type":"markdown","metadata":{"id":"mTUxSU9be8da"},"source":["<a id=\"Pipeline\"></a>\n","\n","## 5. RAG Pipeline Functions\n","We’ll create a function that:\n","1. Retrieves the top doc(s) from FAISS,\n","2. Appends them as context,\n","3. Calls our **LLM** to generate an answer.\n"],"id":"mTUxSU9be8da"},{"cell_type":"code","metadata":{"id":"A9TyQX3Oe8da"},"source":["def rag_prompt(user_query, top_k=3):\n","    # Retrieve the top matching document(s) from FAISS\n","    top_docs = faiss_search(user_query, k=top_k)\n","    context = \"\\n\\n\".join(top_docs)\n","\n","    # Build the RAG prompt with the retrieved context\n","    prompt = f\"\"\"\n","    User's Question: {user_query}\n","\n","    You are a knowledgeable and detail-oriented academic assistant.\n","    Using information provided in the context below, answer the user's question as accurately as possible.\"\n","\n","    Context:\n","    {context}\n","\n","    Answer:\n","    \"\"\"\n","    return generate_llm_text(prompt, max_length=250, temperature=0.2)\n","\n","# Quick test with a domain-relevant query\n","sample_query = \"What recent advances have been made in quantum computing?\"\n","rag_answer = rag_prompt(sample_query)\n","print(\"=== RAG Answer ===\\n\", rag_answer)\n"],"execution_count":null,"outputs":[],"id":"A9TyQX3Oe8da"},{"cell_type":"markdown","metadata":{"id":"iTs1k5wZe8da"},"source":["<a id=\"Compare\"></a>\n","\n","## 6. Console Test: Compare No-RAG vs. RAG\n","To emphasise the difference, we’ll take the **same** user query and:\n","1. **No RAG**: Just feed the query to the LLM with no additional context.\n","2. **RAG**: Retrieve relevant doc and feed it to the LLM.\n"],"id":"iTs1k5wZe8da"},{"cell_type":"code","metadata":{"tags":["activity"],"id":"qZxXIKkhe8da"},"source":["query_test = \"What recent advances have been made in quantum computing?\"\n","\n","# 1) No RAG prompt\n","no_rag_output = generate_llm_text(query_test, max_length=250, temperature=0.2)\n","\n","# 2) RAG-based prompt\n","rag_output = rag_prompt(query_test)\n","\n","print(\"=== WITHOUT RAG ===\\n\")\n","print(no_rag_output)\n","print(\"\\n=== WITH RAG ===\\n\")\n","print(rag_output)"],"execution_count":null,"outputs":[],"id":"qZxXIKkhe8da"},{"cell_type":"markdown","metadata":{"id":"PXs3Jgvfe8da"},"source":["**Activity**: Try various queries that might appear in your domain. Check how the **RAG** approach references actual context from the dataset, whereas the no-RAG approach just relies on the model’s parametric knowledge."],"id":"PXs3Jgvfe8da"},{"cell_type":"markdown","metadata":{"id":"nB7ONHo7e8da"},"source":["<a id=\"Gradio\"></a>\n","\n","## 7. Gradio UI: Interactive RAG Chat\n","We’ll create a chat-like interface. For each user message, we’ll do:\n","1. Retrieve docs from FAISS,\n","2. Generate an LLM response with context.\n","\n","```bash\n","pip install gradio\n","```"],"id":"nB7ONHo7e8da"},{"cell_type":"code","source":["%%capture\n","!pip install gradio"],"metadata":{"id":"43WJIEwmrqLk"},"id":"43WJIEwmrqLk","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRNAFexhe8db"},"source":["import gradio as gr\n","\n","def rag_chat(query):\n","    return rag_prompt(query, top_k=2)\n","\n","demo = gr.Interface(\n","    fn=rag_chat,\n","    inputs=\"text\",\n","    outputs=\"text\",\n","    title=\"Production-Like RAG Demo\",\n","    description=\"Enter a query. We'll retrieve context from arXiv and provide an answer via Flan-T5.\",\n",")\n","\n","# Uncomment to launch the Gradio UI\n","demo.launch(debug=False)"],"execution_count":null,"outputs":[],"id":"XRNAFexhe8db"},{"cell_type":"markdown","metadata":{"id":"1aM5rlxKe8db"},"source":["<a id=\"WrapUp\"></a>\n","\n","## 8. Wrap-Up\n","In this notebook, we:\n","- Used a **larger LLM** (Flan-T5) for “production-like” performance.\n","- Built a **local FAISS** index from a Hugging Face dataset.\n","- Demonstrated **RAG** queries that retrieve relevant text from the dataset.\n","- Compared **no-RAG** vs. **RAG** answers.\n","- Offered an **interactive Gradio** chat.\n","\n","**Next Steps**:\n","1. Explore **bigger or more domain-specific** text corpora.\n","2. Migrate from local FAISS to a **hosted vector DB**.\n","3. Fine-tune or quantise your LLM to reduce VRAM or improve domain accuracy.\n","4. Deploy the Gradio app behind an **API gateway** or container for real-world usage.\n","\n","---\n","# End of Day 3 – Workshop 5 (Production-Like RAG) Notebook\n"],"id":"1aM5rlxKe8db"}]}